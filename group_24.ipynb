{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>** Group 24 **</h2>\n",
    "\n",
    "    1. Ashutosh Mishra(2023aa05913)\n",
    "    2. Waquar Haseeb(2023aa05970\n",
    "    3. Ashish Verma(2023aa05919)\n",
    "    4. Swati Agarwal(2023aa05819)\n",
    "    5. Aditya Mittal(2023aa05589)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now here i have downloaded the models of marker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download datalab-to/surya_layout --cache-dir ./local_models/\n",
    "!huggingface-cli download datalab-to/texify --cache-dir ./local_models/\n",
    "!huggingface-cli download vikp/surya_rec2 --cache-dir ./local_models/\n",
    "!huggingface-cli download datalab-to/surya_tablerec --cache-dir ./local_models/\n",
    "!huggingface-cli download vikp/surya_det3 --cache-dir ./local_models/\n",
    "!huggingface-cli download datalab-to/inline_math_det0 --cache-dir ./local_models/\n",
    "!uggingface-cli download datalab-to/inline_math_rec --cache-dir ./local_models/\n",
    "!huggingface-cli download datalab-to/surya_ocr --cache-dir ./local_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API For the PDF to MarkDown file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "from marker.config.parser import ConfigParser\n",
    "from io import BytesIO\n",
    "import atexit\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Clean up leaked multiprocessing resources\n",
    "def cleanup_resources():\n",
    "    multiprocessing.active_children()  # Ensure child processes are cleaned up\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress warnings\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Base Directory for File Storage\n",
    "BASE_DIR = r\"D:\\WorkSpace_0\\CAI\\infrence\\ConvData\\MarkerData\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "# Milvus Configuration\n",
    "MILVUS_HOST = \"127.0.0.1\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "\n",
    "# Embedding Model (Local Path) - Use your retriever model\n",
    "RETRIEVER_MODEL_DIR = r\"D:\\WorkSpace_0\\CAI\\infrence\\retriever_model\"\n",
    "\n",
    "# Load the retriever model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(RETRIEVER_MODEL_DIR)\n",
    "model = AutoModel.from_pretrained(RETRIEVER_MODEL_DIR)\n",
    "\n",
    "# Calculate vector dimension from the model\n",
    "with torch.no_grad():\n",
    "    # Get sample embedding to determine dimension\n",
    "    sample_inputs = tokenizer(\"This is a test\", return_tensors=\"pt\")\n",
    "    sample_outputs = model(**sample_inputs)\n",
    "    # The hidden state of the [CLS] token is often used as sentence embedding\n",
    "    VECTOR_DIMENSION = sample_outputs.last_hidden_state[:, 0, :].shape[1]\n",
    "    print(f\"Using vector dimension: {VECTOR_DIMENSION}\")\n",
    "\n",
    "# Set Hugging Face Local Model Path\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\WorkSpace_0\\CAI\\infrence\\local_models_marker\"\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Create embeddings using the retriever model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use mean pooling to get the sentence embedding\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return embedding[0].cpu().numpy()\n",
    "\n",
    "def get_collection_name(username, project_name):\n",
    "    \"\"\"Generate a unique collection name using NV_username_projectname.\"\"\"\n",
    "    return f\"NV_{username}_{project_name}\"\n",
    "\n",
    "def connect_to_milvus():\n",
    "    \"\"\"Connect to Milvus.\"\"\"\n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "def create_collection(collection_name):\n",
    "    \"\"\"Create Milvus collection if not exists.\"\"\"\n",
    "    if not utility.has_collection(collection_name):\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=8192),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_DIMENSION)\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, description=f\"Collection for {collection_name}\")\n",
    "        collection = Collection(collection_name, schema)\n",
    "        collection.create_index(\n",
    "            field_name=\"embedding\",\n",
    "            index_params={\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 128}}\n",
    "        )\n",
    "        collection.load()\n",
    "        return collection\n",
    "    else:\n",
    "        collection = Collection(collection_name)\n",
    "        collection.load()\n",
    "        return collection\n",
    "\n",
    "def convert_pdf_to_md(pdf_bytes, output_md_path, output_img_dir):\n",
    "    \"\"\"Converts PDF to Markdown and saves extracted images.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_img_dir, exist_ok=True)\n",
    "    \n",
    "    config_dict = {\n",
    "        \"output_format\": \"markdown\",\n",
    "        \"use_llm\": False,\n",
    "        \"force_ocr\": False,\n",
    "        \"strip_existing_ocr\": False\n",
    "    }\n",
    "    config_parser = ConfigParser(config_dict)\n",
    "    converter = PdfConverter(\n",
    "        config=config_parser.generate_config_dict(),\n",
    "        artifact_dict=create_model_dict(),\n",
    "        processor_list=config_parser.get_processors(),\n",
    "        renderer=config_parser.get_renderer(),\n",
    "        llm_service=config_parser.get_llm_service()\n",
    "    )\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf:\n",
    "        temp_pdf.write(pdf_bytes)\n",
    "        temp_pdf_path = temp_pdf.name\n",
    "\n",
    "    try:\n",
    "        rendered = converter(temp_pdf_path)\n",
    "        text, _, images = text_from_rendered(rendered)\n",
    "\n",
    "        for img_name, img_obj in images.items():\n",
    "            img_path = os.path.join(output_img_dir, f\"{img_name}.png\")\n",
    "            img_buffer = BytesIO()\n",
    "            img_obj.save(img_buffer, format=\"PNG\")\n",
    "            with open(img_path, \"wb\") as img_file:\n",
    "                img_file.write(img_buffer.getvalue())\n",
    "            # Replace image placeholders in Markdown with correct path\n",
    "            text = text.replace(f\"![]({img_name})\", f\"![]({img_path})\")\n",
    "\n",
    "        # Improve table formatting (Markdown tables)\n",
    "        text = fix_markdown_tables(text)\n",
    "\n",
    "        with open(output_md_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "            md_file.write(text)\n",
    "        print(f\"Converted PDF to Markdown: {output_md_path} with images in {output_img_dir}\")\n",
    "        return text # Return Markdown text for API response if needed\n",
    "    finally:\n",
    "        # Cleanup temp file\n",
    "        if os.path.exists(temp_pdf_path):\n",
    "            os.remove(temp_pdf_path)\n",
    "\n",
    "def fix_markdown_tables(text):\n",
    "    \"\"\"Ensure tables are formatted correctly for Markdown.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    new_lines = []\n",
    "    inside_table = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"|\" in line:  # Detect table rows\n",
    "            if not inside_table:\n",
    "                inside_table = True\n",
    "                # Add header formatting if missing\n",
    "                new_lines.append(line)\n",
    "                new_lines.append(\"|---\" * (line.count(\"|\") - 1) + \"|\")\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "        else:\n",
    "            inside_table = False\n",
    "            new_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(new_lines)\n",
    "\n",
    "@app.post(\"/convert\")\n",
    "async def convert_pdf(\n",
    "    username: str = Form(...),\n",
    "    project_name: str = Form(...),\n",
    "    file: UploadFile = File(...)\n",
    "):\n",
    "    \"\"\"Converts PDF to Markdown and saves metadata.\"\"\"\n",
    "    pdf_bytes = await file.read()\n",
    "\n",
    "    # Define user, project, and parsed directories\n",
    "    user_dir = Path(BASE_DIR) / username\n",
    "    project_dir = user_dir / project_name\n",
    "    parsed_dir = user_dir / f\"{project_name}_parsed\"\n",
    "\n",
    "    # Ensure directories exist    \n",
    "    project_dir.mkdir(parents=True, exist_ok=True)\n",
    "    parsed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define file paths\n",
    "    pdf_path = project_dir / file.filename # Original PDF location\n",
    "    base_name = os.path.splitext(file.filename)[0] # Extract file name without extension\n",
    "    output_md_path = parsed_dir / f\"{base_name}.md\" # Markdown file\n",
    "    output_img_dir = parsed_dir / f\"{base_name}_images\" # Images directory\n",
    "    output_img_dir.mkdir(exist_ok=True) # Ensure image directory exists\n",
    "\n",
    "    # Save the uploaded PDF\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(pdf_bytes)\n",
    "\n",
    "    # Call the function to convert PDF to Markdown\n",
    "    markdown_text = convert_pdf_to_md(pdf_bytes, output_md_path, output_img_dir)\n",
    "\n",
    "    return {\n",
    "        \"message\": \"Conversion successful!\",\n",
    "        \"pdf_saved_at\": str(pdf_path),\n",
    "        \"markdown_saved_at\": str(output_md_path),\n",
    "        \"images_saved_in\": str(output_img_dir),\n",
    "        \"markdown_content\": markdown_text\n",
    "    }\n",
    "\n",
    "# Register cleanup at exit\n",
    "atexit.register(cleanup_resources)\n",
    "\n",
    "@app.post(\"/index\")\n",
    "async def index_markdown(username: str, project_name: str):\n",
    "    \"\"\"Indexes Markdown content in Milvus after PDF conversion.\"\"\"\n",
    "    user_dir = Path(BASE_DIR) / username\n",
    "    parsed_dir = user_dir / f\"{project_name}_parsed\"\n",
    "\n",
    "    md_files = list(parsed_dir.glob(\"*.md\"))\n",
    "    if not md_files:\n",
    "        return {\"error\": \"No Markdown files found for indexing\"}\n",
    "\n",
    "    connect_to_milvus()\n",
    "    collection_name = get_collection_name(username, project_name)\n",
    "    \n",
    "    # Check if collection already exists with different dimensions\n",
    "    if utility.has_collection(collection_name):\n",
    "        old_collection = Collection(collection_name)\n",
    "        old_schema = old_collection.schema\n",
    "        old_dim = None\n",
    "        for field in old_schema.fields:\n",
    "            if field.name == \"embedding\":\n",
    "                old_dim = field.params.get(\"dim\")\n",
    "                break\n",
    "                \n",
    "        # If dimensions don't match, drop and recreate the collection\n",
    "        if old_dim is not None and old_dim != VECTOR_DIMENSION:\n",
    "            print(f\"Dropping collection {collection_name} due to dimension mismatch (old: {old_dim}, new: {VECTOR_DIMENSION})\")\n",
    "            utility.drop_collection(collection_name)\n",
    "    \n",
    "    collection = create_collection(collection_name)\n",
    "\n",
    "    for md_file in md_files:\n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            markdown_text = f.read()\n",
    "\n",
    "        text_chunks = [chunk.strip() for chunk in markdown_text.split(\"\\n\\n\") if len(chunk.strip()) > 10]\n",
    "        \n",
    "        # Process chunks in batches to avoid memory issues\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(text_chunks), batch_size):\n",
    "            batch_chunks = text_chunks[i:i+batch_size]\n",
    "            batch_embeddings = [embed_text(chunk).tolist() for chunk in batch_chunks]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            print(f\"Processed batch {i//batch_size + 1}/{(len(text_chunks) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "        insert_data = [text_chunks, all_embeddings]\n",
    "        collection.insert(insert_data)\n",
    "        collection.flush()\n",
    "\n",
    "    return {\"message\": f\"Indexed {len(md_files)} Markdown files in {collection_name}\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=10001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have Download the RETRIEVER_MODEL and RERANKER_MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Define model paths\n",
    "RETRIEVER_MODEL = \"facebook/contriever\"  # Alternative: \"thenlper/gte-large\"\n",
    "# Using a confirmed public reranker model instead\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L6-v2\"  # Alternative to the original model\n",
    "\n",
    "# Set local directories\n",
    "retriever_dir = \"retriever_model\"\n",
    "reranker_dir = \"reranker_model\"\n",
    "\n",
    "print(f\"Downloading and saving retriever model: {RETRIEVER_MODEL}\")\n",
    "\n",
    "# Download and save retriever model and tokenizer\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained(RETRIEVER_MODEL)\n",
    "retriever_model = AutoModel.from_pretrained(RETRIEVER_MODEL)\n",
    "\n",
    "# Save retriever model and tokenizer\n",
    "retriever_tokenizer.save_pretrained(retriever_dir)\n",
    "retriever_model.save_pretrained(retriever_dir)\n",
    "\n",
    "print(f\"Retriever model saved to: {retriever_dir}\")\n",
    "\n",
    "print(f\"Downloading and saving reranker model: {RERANKER_MODEL}\")\n",
    "\n",
    "# Download and save reranker model and tokenizer\n",
    "# Add your Hugging Face token here\n",
    "HF_TOKEN = \"\"  # Replace with your actual token\n",
    "\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL, token=HF_TOKEN)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL, token=HF_TOKEN)\n",
    "\n",
    "# Save reranker model and tokenizer\n",
    "reranker_tokenizer.save_pretrained(reranker_dir)\n",
    "reranker_model.save_pretrained(reranker_dir)\n",
    "\n",
    "print(f\"Reranker model saved to: {reranker_dir}\")\n",
    "\n",
    "print(\"Both models have been successfully downloaded and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I have Download the PHI-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Download the model files - this will save them to your local cache\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\", \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Step 2: Save the model and tokenizer to a local directory\n",
    "local_model_path = \"./phi-2-local\"\n",
    "model.save_pretrained(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {local_model_path}\")\n",
    "\n",
    "# Step 3: Load the model from the local directory\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "local_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test the locally loaded model\n",
    "prompt = \"Once upon a time,\"\n",
    "inputs = local_tokenizer(prompt, return_tensors=\"pt\").to(local_model.device)\n",
    "outputs = local_model.generate(**inputs, max_length=50)\n",
    "print(local_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API For the infrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Query\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "import torch\n",
    "from pymilvus import connections, Collection\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "import re\n",
    "import spacy\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configurations\n",
    "PHI_2_MODEL_DIR = \"D:\\\\WorkSpace_0\\\\CAI\\\\infrence\\\\phi-2-local\"\n",
    "RETRIEVER_MODEL_DIR = \"D:\\\\WorkSpace_0\\\\CAI\\\\infrence\\\\retriever_model\"\n",
    "RERANKER_MODEL_DIR = \"D:\\\\WorkSpace_0\\\\CAI\\\\infrence\\\\reranker_model\"\n",
    "MIN_QUERY_LENGTH = 5\n",
    "RESTRICTED_WORDS_FILE = \"restricted_words.txt\"\n",
    "PROFANITY_LIST = [\"badword1\", \"badword2\"]  # Expand this list as needed\n",
    "DEFAULT_TOP_K = 5\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='guardrail_logs.log', level=logging.INFO)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Global variables for models\n",
    "tokenizer = None\n",
    "model = None\n",
    "retriever_model = None\n",
    "retriever_tokenizer = None\n",
    "reranker_model = None\n",
    "nlp = None\n",
    "\n",
    "# Load models at startup\n",
    "@app.on_event(\"startup\")\n",
    "def load_models():\n",
    "    \"\"\"Load all the necessary models at startup.\"\"\"\n",
    "    global tokenizer, model, retriever_model, retriever_tokenizer, reranker_model, nlp\n",
    "    \n",
    "    # Load LLM for answer generation\n",
    "    tokenizer, model = load_phi2_model()\n",
    "    \n",
    "    # Load retriever model\n",
    "    retriever_tokenizer, retriever_model = load_retriever_model()\n",
    "    \n",
    "    # Load reranker model\n",
    "    reranker_model = load_reranker_model()\n",
    "    \n",
    "    # Load NLP model for entity recognition\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define request model\n",
    "class QueryRequest(BaseModel):\n",
    "    username: str\n",
    "    project_name: str\n",
    "    query: str\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    "\n",
    "# API endpoint\n",
    "@app.post(\"/generate\")\n",
    "async def generate_response_endpoint(request: QueryRequest):\n",
    "    \"\"\"Generate a response based on the user's query using retrieval and reranking.\"\"\"\n",
    "    try:\n",
    "        response_data = main(\n",
    "            request.username,\n",
    "            request.project_name,\n",
    "            request.query,\n",
    "            request.top_k,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            retriever_model,\n",
    "            retriever_tokenizer,\n",
    "            reranker_model,\n",
    "            nlp\n",
    "        )\n",
    "        return response_data\n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing request: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "# Core processing function\n",
    "def main(username, project_name, query, top_k, tokenizer, model, retriever_model, retriever_tokenizer, reranker_model, nlp):\n",
    "    \"\"\"Process the query and generate a response with context, query, response, and references.\"\"\"\n",
    "    connect_to_milvus()\n",
    "    collection_name = get_collection_name(username, project_name)\n",
    "    collection = Collection(collection_name)\n",
    "    collection.load()\n",
    "\n",
    "    query = validate_query(query)\n",
    "    \n",
    "    # Create embeddings using the retriever model\n",
    "    query_embedding = embed_query(query, retriever_model, retriever_tokenizer)\n",
    "    \n",
    "    # Initial retrieval with embeddings\n",
    "    retrieved_docs, retrieval_scores = retrieve_documents(collection, query_embedding, query, top_k * 2)\n",
    "    \n",
    "    # Rerank the retrieved documents\n",
    "    reranked_docs, confidence_scores = rerank_documents(reranker_model, query, retrieved_docs, top_k)\n",
    "    \n",
    "    # Extract sources from the reranked documents\n",
    "    sources = [extract_source(doc) for doc in reranked_docs]\n",
    "    unique_sources = list(set(sources))\n",
    "    \n",
    "    # Prepare context from the reranked documents\n",
    "    context = \"\\n\".join(reranked_docs)\n",
    "\n",
    "    # Truncate context if too long\n",
    "    if len(tokenizer.encode(context)) > 1500:\n",
    "        context = tokenizer.decode(tokenizer.encode(context)[:1500])\n",
    "\n",
    "    prompt = (\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Only provide the exact answer to the question without additional information.\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Generate the response\n",
    "    answer = generate_response(model, tokenizer, prompt)\n",
    "    final_response = filter_response(answer, nlp)\n",
    "\n",
    "    return {\n",
    "        \"reference_text\": context,\n",
    "        \"query\": query,\n",
    "        \"response\": final_response,\n",
    "        \"reference_doc\": unique_sources,\n",
    "        \"confidence_score\": max(confidence_scores) if confidence_scores else 0.0\n",
    "    }\n",
    "\n",
    "# Helper functions\n",
    "def get_collection_name(username, project_name):\n",
    "    \"\"\"Generate a unique collection name based on username and project.\"\"\"\n",
    "    return f\"NV_{username}_{project_name}\"\n",
    "\n",
    "def connect_to_milvus():\n",
    "    \"\"\"Establish a connection to the Milvus server.\"\"\"\n",
    "    connections.connect(\"default\", host=\"127.0.0.1\", port=\"19530\")\n",
    "\n",
    "def load_retriever_model():\n",
    "    \"\"\"Load the retriever model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(RETRIEVER_MODEL_DIR)\n",
    "    model = AutoModel.from_pretrained(RETRIEVER_MODEL_DIR)\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_reranker_model():\n",
    "    \"\"\"Load the reranker model.\"\"\"\n",
    "    return CrossEncoder(RERANKER_MODEL_DIR)\n",
    "\n",
    "def embed_query(query, retriever_model, retriever_tokenizer):\n",
    "    \"\"\"Create embedding for the query using the retriever model.\"\"\"\n",
    "    inputs = retriever_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = retriever_model(**inputs)\n",
    "        # Use mean pooling to get the sentence embedding\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        # Get the embedding as a numpy array\n",
    "        embedding_np = embedding[0].cpu().numpy()\n",
    "        \n",
    "        # Check dimension compatibility with your original model\n",
    "        # If your original SentenceTransformer model produced 384-dim vectors (common for all-MiniLM-L6-v2)\n",
    "        expected_dim = 384  # Replace with the dimension of your Milvus collection\n",
    "        actual_dim = embedding_np.shape[0]\n",
    "        \n",
    "        # Handle dimension mismatch\n",
    "        if actual_dim != expected_dim:\n",
    "            if actual_dim > expected_dim:\n",
    "                # Truncate if new embedding is larger\n",
    "                embedding_np = embedding_np[:expected_dim]\n",
    "            else:\n",
    "                # Pad with zeros if new embedding is smaller\n",
    "                padding = np.zeros(expected_dim - actual_dim)\n",
    "                embedding_np = np.concatenate([embedding_np, padding])\n",
    "        \n",
    "        return embedding_np.tolist()\n",
    "\n",
    "def retrieve_documents(collection, query_embedding, query_text, limit=10):\n",
    "    \"\"\"Retrieve documents using vector search.\"\"\"\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    results = collection.search(\n",
    "        [query_embedding],\n",
    "        \"embedding\",\n",
    "        search_params,\n",
    "        limit=limit,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = [hit.entity.get(\"text\") for hit in results[0]]\n",
    "    retrieval_scores = [hit.score for hit in results[0]]\n",
    "    \n",
    "    return retrieved_docs, retrieval_scores\n",
    "\n",
    "def rerank_documents(reranker_model, query, documents, top_k=5):\n",
    "    \"\"\"Rerank the retrieved documents using the reranker model.\"\"\"\n",
    "    if not documents:\n",
    "        return [], []\n",
    "    \n",
    "    # Prepare input pairs for reranking\n",
    "    pairs = [(query, doc) for doc in documents]\n",
    "    \n",
    "    # Get scores from the reranker model\n",
    "    scores = reranker_model.predict(pairs)\n",
    "    \n",
    "    # Sort documents by reranker scores\n",
    "    ranked_results = [(doc, score) for doc, score in zip(documents, scores)]\n",
    "    ranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top-k documents and their scores\n",
    "    top_docs = [doc for doc, _ in ranked_results[:top_k]]\n",
    "    top_scores = [float(score) for _, score in ranked_results[:top_k]]\n",
    "    \n",
    "    return top_docs, top_scores\n",
    "\n",
    "def extract_source(text):\n",
    "    \"\"\"Extract the source name from the text.\"\"\"\n",
    "    match = re.match(r\"Source:\\s*(.+?)\\n\", text)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "def load_restricted_words():\n",
    "    \"\"\"Load restricted words from a file or return defaults.\"\"\"\n",
    "    if os.path.exists(RESTRICTED_WORDS_FILE):\n",
    "        with open(RESTRICTED_WORDS_FILE, 'r') as f:\n",
    "            return [line.strip().lower() for line in f.readlines()]\n",
    "    return [\"hack\", \"exploit\"]\n",
    "\n",
    "def validate_query(query):\n",
    "    \"\"\"Validate the query for length, restricted words, and SQL injection attempts.\"\"\"\n",
    "    restricted_words = load_restricted_words()\n",
    "    if len(query) < MIN_QUERY_LENGTH:\n",
    "        logging.warning(f\"Query too short: {query}\")\n",
    "        raise ValueError(\"Invalid query: Too short.\")\n",
    "    if any(word in query.lower() for word in restricted_words):\n",
    "        logging.warning(f\"Restricted word found in query: {query}\")\n",
    "        raise ValueError(\"Invalid query: Contains restricted content.\")\n",
    "    sql_injection_patterns = [\n",
    "        r\"\\b(select|insert|update|delete|drop|alter)\\b\",\n",
    "        r\"\\b(union|into|from|where)\\b\",\n",
    "        r\"--|/\\*|\\*/\"\n",
    "    ]\n",
    "    for pattern in sql_injection_patterns:\n",
    "        if re.search(pattern, query, re.IGNORECASE):\n",
    "            logging.warning(f\"Potential SQL injection attempt: {query}\")\n",
    "            raise ValueError(\"Invalid query: Potential SQL injection attempt.\")\n",
    "    return query\n",
    "\n",
    "def filter_response(response, nlp):\n",
    "    \"\"\"Filter the response for profanity and sensitive entities.\"\"\"\n",
    "    if any(word in response.lower() for word in PROFANITY_LIST):\n",
    "        logging.info(f\"Profanity detected in response: {response}\")\n",
    "        return \"[Response blocked due to inappropriate content.]\"\n",
    "    doc = nlp(response)\n",
    "    redacted_response = response\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"]:\n",
    "            redacted_response = redacted_response.replace(ent.text, \"[REDACTED]\")\n",
    "    logging.info(f\"Filtered response: {redacted_response}\")\n",
    "    return redacted_response\n",
    "\n",
    "def load_phi2_model():\n",
    "    \"\"\"Load the Phi-2 model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PHI_2_MODEL_DIR, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        PHI_2_MODEL_DIR,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    \"\"\"Generate a response using the Phi-2 model, returning only the generated part.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    generated_sequence = outputs[0, input_length:]\n",
    "    return tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=10002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
